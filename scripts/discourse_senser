#!/usr/bin/env python
# -*- coding: utf-8; mode: python; -*-

"""Script for doing sense disambiguation of discourse connectives.

"""

##################################################################
# Imports
from __future__ import print_function, unicode_literals

from dsenser import DiscourseSenser, \
    DFLT_MODEL_PATH, DFLT_MODEL_TYPE, \
    FFNN, LSTM, MJR, SVM, WANG

import argparse
import codecs
import glob
import json
import os
import sys


##################################################################
# Variables and Constants
M_TRAIN = "train"
M_TEST = "test"
ENCODING = "utf-8"

ARGS_FNAME = "relations-no-senses.json"
GOLD_FNAME = "relations.json"
PARSE_FNAME = "parses.json"
OUT_FNAME = "output.json"
RAW_DIR = "raw"
WSJ_GLOB="wsj_*"


##################################################################
# Methods
def _add_cmn_options(a_parser):
    """Add common options to option subparser

    Args:
    a_parser (argparse.ArgumentParser):
      option subparser to which new options should be added

    Returns:
    :void

    """
    a_parser.add_argument("-m", "--model",
                          help="path to the main model (if different from default)",
                          type=str, default=DFLT_MODEL_PATH)
    a_parser.add_argument("input_dir", help="directory containing input data",
                          type=str)


def _read_json(a_dir, a_fname, a_enc=ENCODING):
    """Read JSON data from file.

    Args:
    a_dir (str):
      dirname to the input file
    a_fname (str):
      basename of the input file
    a_enc (str):
      input file encoding

    Returns:
    (dict):
    input data as dictionary

    """
    fname = os.path.join(a_dir, a_fname or "")
    if a_fname is None or not (os.path.exists(fname) and
                               os.access(fname, os.R_OK)):
        raise RuntimeError("Can't open file '{:s}'.".format(a_fname))
    print("Loading file '{:s}'... ".format(fname), end="", file=sys.stderr)
    ret = []
    with codecs.open(fname, 'r', encoding=a_enc) as ifile:
        for iline in ifile:
            iline = iline.strip()
            if iline:
                ret.append(json.loads(iline))
    print("done", file=sys.stderr)
    return ret


def _read_raw(a_dir, a_enc=ENCODING):
    """Read raw data from directory.

    Args:
    a_dir (str):
     path to directory containing raw data
    a_enc (str):
     encoding of the input files

    Returns:
    dict(str, tuple(int, int, str)):
      dictionary with file names as keys and 3-tuple (paragraph number,
      sentence numberm, and sentence string) as values

    """
    ret = dict()
    bfname = ""
    snt_cnt = 0
    prgrph_cnt = -1
    prev_empty = False
    sentences = None
    for fname in glob.iglob(os.path.join(a_dir, WSJ_GLOB)):
        snt_cnt = 0
        prgrph_cnt = -1
        bfname = os.path.basename(bfname)
        sentences = ret[bfname] = []
        print("Loading file '{:s}'... ".format(fname), end="", file=sys.stderr)
        with codecs.open(fname, 'r', ENCODING) as ifile:
            for iline in ifile:
                iline = iline.strip()
                if not iline:
                    if prev_empty:
                        continue
                    prev_empty = True
                    prgrph_cnt += 1
                elif iline == ".START":
                    continue
                else:
                    prev_empty = False
                    sentences.append((prgrph_cnt, snt_cnt, iline))
                    snt_cnt += 1
        print("done", file=sys.stderr)
    return ret


def _read_dataset(a_dir, a_train=False):
    """Read data from directory.

    Args:
    a_dir (str):
      path to directory containing input data
    a_train (bool):
      path to directory containing input data

    Returns:
    2-tuple(dict, dict): training rel set and parses

    """
    if a_dir is None:
        return (None, None, None)
    # read relations
    if a_train:
        rels = _read_json(a_dir, GOLD_FNAME)
    else:
        rels = _read_json(a_dir, ARGS_FNAME)
    # read parses
    with codecs.open(os.path.join(a_dir, PARSE_FNAME), 'r', ENCODING) as ifile:
        print("Loading file '{:s}'... ".format(ifile.name), end="",
              file=sys.stderr)
        parses = json.load(ifile)
        print("done", file=sys.stderr)
    return (rels, parses)


def main():
    """Main method for doing discourse parsing of CoNLL JSON data.

    Args:
    (void)
      sys.argv are parsed in place

    Returns:
    (int)
      0 on success, non-0 otherwise

    """
    # parse arguments
    argparser = argparse.ArgumentParser(description="""
    Determine senses of discourse connectives.""")

    subparsers = argparser.add_subparsers(help="type of operation to perform",
                                          dest="mode")
    # training parser
    parser_train = subparsers.add_parser(M_TRAIN, help=
                                         "train new model on provided data")
    parser_train.add_argument("-d", "--dev", help=
                              "development directory with JSON data",
                              type=str)
    parser_train.add_argument("--type", help=("type of the model to train"
                               "({:d} - feed-forward, {:d} - LSTM, {:d} - majority,"
                               " {:d} - SVM, {:d} - Wang), ".format(FFNN, LSTM, MJR,
                                                                   SVM, WANG) +
                               "default = {:d}".format(DFLT_MODEL_TYPE)),
                              type=str, action="append", choices=(FFNN, LSTM, MJR,
                                                                  SVM, WANG),
                              default=[DFLT_MODEL_TYPE])
    _add_cmn_options(parser_train)

    # testing parser
    parser_test = subparsers.add_parser(M_TEST, help="""determine senses
    using pre-trained models""")
    _add_cmn_options(parser_test)
    parser_test.add_argument("output_dir", help=
                              "directory for storing output files", type=str)
    args = argparser.parse_args()

    # instantiate objects required for processing
    dsenser = None

    if args.mode == M_TRAIN:
        # read data
        dev_set = _read_dataset(args.dev, True)
        input_set = _read_dataset(args.input_dir, True)
        # train the model
        # determine model type
        mtype = 0
        for itype in args.type:
            mtype |= itype
        dsenser =  DiscourseSenser(None)
        dsenser.train(input_set, mtype, args.model, dev_set)
    else:
        if not os.path.exists(args.output_dir):
            os.makedirs(args.output_dir)
        elif not os.path.isdir(args.output_dir) or not \
             os.access(args.output_dir, os.W_OK):
                raise RuntimeError("Cannot write to directory '{:s}'.".format(
                    args.output_dir))
        # read test data
        dsenser =  DiscourseSenser(args.model)
        test_set = _read_dataset(args.input_dir)
        dsenser.predict(test_set)
        # write results to file
        ofname = os.path.join(args.output_dir, OUT_FNAME)
        with codecs.open(ofname, 'w', ENCODING) as ofile:
            for irel in test_set[0]:
                print(json.dumps(irel), file=ofile)


##################################################################
# Main
if __name__ == "__main__":
    main()
